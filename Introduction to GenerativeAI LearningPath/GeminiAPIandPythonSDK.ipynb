{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Cloud > VertexAI > Workbench\n",
    "\n",
    "* Gemini 1.0 Pro --- NLP tasks \n",
    "* Gemini 1.0 Pro-vision --- for vision tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"qwiklabs-gcp-01-b1a82f306fd5\"  \n",
    "LOCATION = \"us-central1\" \n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "from vertexai.generative_models import GenerationConfig, GenerativeModel, Image, Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini Pro Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-1.0-pro\")\n",
    "\n",
    "responses = model.generate_content(\"Why is the sky blue?\", stream=True)  # responses will be streamed as they are generated. \n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")  # print the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Create a numbered list of 10 items. Each item in the list should be a trend in the tech industry.\n",
    "\n",
    "Each trend should be less than 5 words.\"\"\"  \n",
    "\n",
    "responses = model.generate_content(prompt, stream=True)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=0.9,  # randomness of the prediction scaling the logits before before applying softmax. higher value increases the randomness \n",
    "    top_p = 1.0, # nucleus sampling, controls the cumulative probability threshold for token selection. top x% probable tokens\n",
    "    top_k = 32, # number of top probable tokens to consider for sampling increasing the coherence by increasing the k\n",
    "    candidate_count = 1, # number of candidate sequences to generate in parallel. \n",
    "    max_output_tokens = 8192,\n",
    ")\n",
    "\n",
    "responses = model.generate_content(\n",
    "    \"Why is the sky blue?\",\n",
    "    generation_config=generation_config,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Chat prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = model.start_chat()\n",
    "\n",
    "prompt = \"\"\"My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.\n",
    "\n",
    "Suggest another movie I might like.\n",
    "\"\"\"\n",
    "\n",
    "responses = chat.send_message(prompt, stream=True)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow up prompts\n",
    "\n",
    "prompt = \"Are my favorite movies based on a book series?\"\n",
    "\n",
    "responses = chat.send_message(prompt, stream=True)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini 1.0 Pro vision model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client  # for http request to interact with web servers\n",
    "import typing # defining types for variables and functions, improving code readability and maintability \n",
    "import urllib.request # tools for working with URLS, potentially for constucting requests to access APIs. \n",
    "\n",
    "import IPython.display  # specific to IPYthon notebooks\n",
    "from PIL import Image as PIL_Image \n",
    "from PIL import ImageOps as PIL_ImageOps # various Image operations and manipulations that might be usefull for processing images generated by the model. \n",
    "\n",
    "# Perfect coding --> var: type = value \n",
    "\n",
    "def display_images(\n",
    "    images: typing.Iterable[Image],\n",
    "    max_width: int = 600,\n",
    "    max_height: int = 350,\n",
    ") -> None:\n",
    "    \n",
    "    # its iterable now\n",
    "    for image in images:\n",
    "        pil_image = typing.cast(PIL_Image.Image, image._pil_image)\n",
    "        \n",
    "        #m converts the image to RGB if not already \n",
    "        if pil_image.mode != \"RGB\":  # check the \"mode\" of the image. \n",
    "            pil_image = pil_image.convert(\"RGB\")\n",
    "            \n",
    "        # resizes the image\n",
    "        image_width, image_height = pil_image.size\n",
    "        if max_width < image_width or max_height < image_height:\n",
    "            pil_image = PIL_ImageOps.contain(pil_image, (max_width, max_height))\n",
    "\n",
    "        # with Ipython.display, diplay the image\n",
    "        IPython.display.display(pil_image)\n",
    "\n",
    "\n",
    "# output the bytes. Fetches image data from the URL\n",
    "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
    "    \n",
    "    # use urllib.request.urlopen()\n",
    "    with urllib.request.urlopen(image_url) as response:\n",
    "        # we need it as http.client.HTTpResponse \n",
    "        response = typing.cast(http.client.HTTPResponse, response)\n",
    "        image_bytes = response.read()  # read the image \n",
    "        \n",
    "    return image_bytes\n",
    "\n",
    "# loads the image from the urls and converts it to an \"Image\" Object \n",
    "def load_image_from_url(image_url: str) -> Image:\n",
    "    image_bytes = get_image_bytes_from_url(image_url)\n",
    "    \n",
    "    # use Image library \n",
    "    return Image.from_bytes(image_bytes)\n",
    "\n",
    "# converts a google cloud storage (GCS) URI to a URL that can be used to fetch the image. \n",
    "def get_url_from_gcs(gcs_uri: str) -> str:   # GCS URI to convert \n",
    "    # converts gcs uri to url for image display.\n",
    "    url = \"https://storage.googleapis.com/\" + gcs_uri.replace(\"gs://\", \"\").replace(\n",
    "        \" \", \"%20\"\n",
    "    )\n",
    "    return url\n",
    "\n",
    "\n",
    "def print_multimodal_prompt(contents: list):\n",
    "    \"\"\"\n",
    "    Given contents that would be sent to Gemini,\n",
    "    output the full multimodal prompt for ease of readability.\n",
    "    \"\"\"\n",
    "    for content in contents:\n",
    "        \n",
    "        # imported from the vertexai.generative_models --> Image and Part \n",
    "        if isinstance(content, Image):  # use isinstance()\n",
    "            display_images([content])\n",
    "            \n",
    "        elif isinstance(content, Part):\n",
    "            url = get_url_from_gcs(content.file_data.file_uri)\n",
    "            IPython.display.display(load_image_from_url(url))\n",
    "        else:\n",
    "            print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an image from Google Cloud Storage\n",
    "! gsutil cp \"gs://cloud-samples-data/generative-ai/image/320px-Felis_catus-cat_on_snow.jpg\" ./image.jpg\n",
    "\n",
    "# Load from local file\n",
    "image = Image.load_from_file(\"image.jpg\")\n",
    "\n",
    "# Prepare contents\n",
    "prompt = \"Describe this image?\"\n",
    "contents = [image, prompt]  # make it as a list \n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image from Cloud Storage URI\n",
    "gcs_uri = \"gs://cloud-samples-data/generative-ai/image/boats.jpeg\"\n",
    "\n",
    "# Prepare contents\n",
    "image = Part.from_uri(gcs_uri, mime_type=\"image/jpeg\")\n",
    "prompt = \"Describe the scene?\"\n",
    "contents = [image, prompt]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image from Cloud Storage URI\n",
    "image_url = (\n",
    "    \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/boats.jpeg\"\n",
    ")\n",
    "image = load_image_from_url(image_url)  # convert to bytes\n",
    "\n",
    "# Prepare contents\n",
    "prompt = \"Describe the scene?\"\n",
    "contents = [image, prompt]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from Cloud Storage URI\n",
    "image1_url = \"https://storage.googleapis.com/github-repo/img/gemini/intro/landmark1.jpg\"\n",
    "image2_url = \"https://storage.googleapis.com/github-repo/img/gemini/intro/landmark2.jpg\"\n",
    "image3_url = \"https://storage.googleapis.com/github-repo/img/gemini/intro/landmark3.jpg\"\n",
    "image1 = load_image_from_url(image1_url)\n",
    "image2 = load_image_from_url(image2_url)\n",
    "image3 = load_image_from_url(image3_url)\n",
    "\n",
    "# Prepare prompts\n",
    "prompt1 = \"\"\"{\"city\": \"London\", \"Landmark:\", \"Big Ben\"}\"\"\"\n",
    "prompt2 = \"\"\"{\"city\": \"Paris\", \"Landmark:\", \"Eiffel Tower\"}\"\"\"\n",
    "\n",
    "# Prepare contents\n",
    "contents = [image1, prompt1, image2, prompt2, image3]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4\"\n",
    "video_uri = f\"gs://{file_path}\"\n",
    "video_url = f\"https://storage.googleapis.com/{file_path}\"\n",
    "\n",
    "IPython.display.Video(video_url, width=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the following questions using the video only:\n",
    "What is the profession of the main person?\n",
    "What are the main features of the phone highlighted?\n",
    "Which city was this recorded in?\n",
    "Provide the answer JSON.\n",
    "\"\"\"\n",
    "\n",
    "video = Part.from_uri(video_uri, mime_type=\"video/mp4\")\n",
    "contents = [prompt, video]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
